{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process huge json file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize\n",
    "import pyarrow as pa\n",
    "\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "\n",
    "# Set up a logger to dump messages to both log file and notebook\n",
    "import logging as logging\n",
    "def ini_log(filename):\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    \n",
    "    handlers = [logging.StreamHandler(None), logging.FileHandler(filename, 'a')]\n",
    "    \n",
    "    fmt=logging.Formatter('%(asctime)-15s: %(levelname)s  %(message)s')\n",
    "    for h in handlers:\n",
    "        h.setFormatter(fmt)\n",
    "        logger.addHandler(h)\n",
    "    return logger\n",
    "        \n",
    "log = ini_log('out.log')\n",
    "#log.basicConfig(filename='out.log',level=log.DEBUG, format='%(asctime)-15s: %(levelname)s  %(message)s')\n",
    "\n",
    "import gc\n",
    "gc.enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['models', 'sample_submission_v2.csv', 'test_v2.csv', 'tmp', 'train_v2.csv']\n"
     ]
    }
   ],
   "source": [
    "def_num = np.nan\n",
    "def_str = 'NaN'\n",
    "\n",
    "def get_keys_for_field(field=None):\n",
    "    the_dict = {\n",
    "        'device': [\n",
    "            'browser', 'object',\n",
    "            'deviceCategory',\n",
    "            ('isMobile', False, bool),\n",
    "            'operatingSystem'\n",
    "        ],\n",
    "        'geoNetwork': [\n",
    "            'city',\n",
    "            'continent',\n",
    "            'country',\n",
    "            'metro',\n",
    "            'networkDomain',\n",
    "            'region',\n",
    "            'subContinent'\n",
    "        ],\n",
    "        'totals': [\n",
    "            ('pageviews', 0, np.int16),\n",
    "            ('hits', def_num, np.int16),\n",
    "            ('bounces', 0, np.int8),\n",
    "            ('newVisits', 0, np.int16),\n",
    "            ('totalTransactionRevenue', 0, np.int64),\n",
    "            ('visits', -1, np.int16),\n",
    "            ('timeOnSite', -1, np.int32),\n",
    "            ('sessionQualityDim', -1, np.int8),\n",
    "        ],\n",
    "        'trafficSource': [\n",
    "            'adContent',\n",
    "            #'adwordsClickInfo',\n",
    "            'campaign',\n",
    "            ('isTrueDirect', False, bool),\n",
    "            #'keyword', #can not be saved in train (utf-8 symbols left)\n",
    "            'medium',\n",
    "            'referralPath',\n",
    "            'source'\n",
    "        ],\n",
    "    }\n",
    "    return the_dict[field]\n",
    "\n",
    "\n",
    "def convert_to_dict(x):\n",
    "    #print(x, type(x))\n",
    "    return eval(x.replace('false', 'False')\n",
    "                .replace('true', 'True')\n",
    "                .replace('null', 'np.nan'))\n",
    "\n",
    "def develop_json_fields(fin, json_fields=['totals'], bsize=1e8, cols_2drop=[]):\n",
    "    df = dd.read_csv(fin, blocksize=bsize, \n",
    "                 #converters={column: json.loads for column in JSON_COLUMNS},\n",
    "                 dtype={'fullVisitorId': 'str', # Important!!\n",
    "                        #usecols=lambda c: c not in cols_2drop,\n",
    "                            'date': 'str',\n",
    "                            **{c: 'str' for c in json_fields}\n",
    "                           },\n",
    "                     parse_dates=['date'],)#.head(10000, 100)\n",
    "    \n",
    "    df = df.drop(cols_2drop, axis=1)\n",
    "    \n",
    "    # Get the keys\n",
    "    for json_field in json_fields:\n",
    "        log.info('Doing Field {}'.format(json_field))\n",
    "        # Get json field keys to create columns\n",
    "        the_keys = get_keys_for_field(json_field)\n",
    "        # Replace the string by a dict\n",
    "        log.info('Transform string to dict')        \n",
    "        df[json_field] = df[json_field].apply(lambda x: convert_to_dict(x), meta=('','object'))\n",
    "        \n",
    "        log.info('{} converted to dict'.format(json_field))\n",
    "        #display(df.head())\n",
    "        for k in the_keys:\n",
    "            if isinstance(k, str):\n",
    "                t_ = def_str\n",
    "                k_ = k\n",
    "            else:\n",
    "                t_ = k[1]\n",
    "                k_ = k[0]\n",
    "            df[json_field + '_' + k_] = df[json_field].to_bag().pluck(k_, default=t_).to_dataframe().iloc[:,0]\n",
    "            if not isinstance(k, str) and len(k)>2:\n",
    "                df[json_field + '_' + k_] = df[json_field + '_' + k_].astype(k[2])\n",
    "            \n",
    "        del df[json_field]\n",
    "        gc.collect()\n",
    "        log.info('{} fields extracted'.format(json_field))\n",
    "    return df\n",
    "\n",
    "print(os.listdir(\"data/kg-google/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "JSON_COLUMNS = ['device', 'geoNetwork', 'totals', 'trafficSource']\n",
    "DROP_COLUMNS = ['customDimensions', 'hits', 'socialEngagementType']\n",
    "\n",
    "def measure_memory(df, name):\n",
    "    size_df = df.memory_usage(deep=True)\n",
    "    log.info('{} size: {:.2f} MB'.format(name, size_df.sum().compute()/ 1024**2))\n",
    "    \n",
    "def read_parse_store(fin, label='XXX', bsize=1e9):\n",
    "    log.debug('Start with {}'.format(label))\n",
    "    df_  = develop_json_fields(fin,  bsize=bsize, json_fields=JSON_COLUMNS, cols_2drop=DROP_COLUMNS)\n",
    "    \n",
    "    #some stats\n",
    "    measure_memory(df_, label)\n",
    "    log.info('Number of partitions in {}: {}'.format(label, df_.npartitions))\n",
    "    \n",
    "    #visualize a few rows\n",
    "    display(df_.head())\n",
    "    \n",
    "    #reduce var size\n",
    "    df_['visitNumber'] = df_['visitNumber'].astype(np.uint16)\n",
    "\n",
    "    #read the whole dataset into pd.DataFrame in memory and store into a single file\n",
    "    #otherwise dask.DataFrame would be stored into multiple files- 1 per partition\n",
    "    df_.compute().to_csv(\"{}-flat.csv.gz\".format(label), index=False , compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-23 20:18:05,354: DEBUG  Start with train\n",
      "2018-11-23 20:18:05,658: INFO  Doing Field device\n",
      "2018-11-23 20:18:05,659: INFO  Transform string to dict\n",
      "2018-11-23 20:18:05,662: INFO  device converted to dict\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'iloc'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-a97417eefd45>\u001b[0m in \u001b[0;36mread_parse_store\u001b[1;34m(fin, label, bsize)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mread_parse_store\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'XXX'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e9\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mlog\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Start with {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mdf_\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mdevelop_json_fields\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mbsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbsize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjson_fields\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mJSON_COLUMNS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcols_2drop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDROP_COLUMNS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;31m#some stats\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-a276b947c698>\u001b[0m in \u001b[0;36mdevelop_json_fields\u001b[1;34m(fin, json_fields, bsize, cols_2drop)\u001b[0m\n\u001b[0;32m     79\u001b[0m                 \u001b[0mt_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m                 \u001b[0mk_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m             \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mjson_field\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'_'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mk_\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mjson_field\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_bag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpluck\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mt_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_dataframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m                 \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mjson_field\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'_'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mk_\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mjson_field\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'_'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mk_\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\python\\lib\\site-packages\\dask\\dataframe\\core.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2412\u001b[0m             return new_dd_object(merge(self.dask, dsk), name,\n\u001b[0;32m   2413\u001b[0m                                  meta, self.divisions)\n\u001b[1;32m-> 2414\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"'DataFrame' object has no attribute %r\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2415\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2416\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__dir__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'iloc'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "read_parse_store('data/kg-google/train_v2.csv', 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
